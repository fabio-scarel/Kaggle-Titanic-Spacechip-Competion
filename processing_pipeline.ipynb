{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"read_data\"></a> Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (Path('..') / 'Kaggle-Titanic-Spacechip-Competion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path / 'train.csv')\n",
    "test_data = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Transported']\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "train_data = train_data.drop(columns=['Transported'])\n",
    "train_data = train_data.drop(columns=['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(train_data, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The name of the passengers isn't going to be used for nothing in this notebook\n",
    "# train_data_1 = train_data.drop('Name', axis=1)\n",
    "# test_data_1 = test_data.drop('Name',axis=1)\n",
    "# # The passenger IDs will be used at the end to return the correct IDs with the model previsions\n",
    "# passenger_id = train_data_1['PassengerId']\n",
    "# test_passenger_id = test_data_1['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import feature_engine.imputation as mdi\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.discretisation import DecisionTreeDiscretiser\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "from feature_engine.transformation import PowerTransformer  as fe_PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer as sk_PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class BooleanTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return list(input_features)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['VIP'] = X['VIP'].map({True: 1, False: 0})\n",
    "        X['CryoSleep'] = X['CryoSleep'].map({True: 1, False: 0})\n",
    "        return X\n",
    "\n",
    "class AddPassengerGroup(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['passenger_group'] = X['PassengerId'].str[:4]\n",
    "        X = X.drop(columns=['PassengerId'], axis=1)\n",
    "        return X\n",
    "\n",
    "class FillBinaryNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        binary_columns = ['CryoSleep','VIP']\n",
    "        numeric_columns = ['FoodCourt','RoomService','Spa','VRDeck','ShoppingMall']\n",
    "\n",
    "        # I am assuming here that if there is no record of the person in CryoSleep or if they are on the VIP list they probably aren't in neither\n",
    "        X.loc[:, binary_columns] = X[binary_columns].fillna(False)\n",
    "        # The same logic applies here, if there is no record of the passenger spending money, they probably didn't spend it\n",
    "        X.loc[:, numeric_columns] = X[numeric_columns].fillna(0)\n",
    "        return X\n",
    "\n",
    "class FillCabinDestHomeAgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Here the function groups the data by the passanger group, and fills in the other data based on the info of another passanger that's in their group\n",
    "        # For example if the daughter didn't put her data correct but her mother did, the code will retrieve that information\n",
    "        X['Cabin'] = X.groupby('passenger_group')['Cabin'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Destination'] = X.groupby('passenger_group')['Destination'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['HomePlanet'] = X.groupby('passenger_group')['HomePlanet'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        return X\n",
    "\n",
    "class FillRestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # For the groups that didn't have any information, I just decided to fill in the Age with the mean of values from that group and the other just a 'missing' string\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X['Age'] = X['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X[['Cabin','HomePlanet','Destination']] = X[['Cabin','HomePlanet','Destination']].fillna('missing')\n",
    "        return X\n",
    "\n",
    "class DualCategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 fill_value=\"missing\",\n",
    "                 imputation_method=\"most_frequent\"):\n",
    "\n",
    "        self.fill_value = fill_value\n",
    "        self.imputation_method = imputation_method\n",
    "        self.imputer_1 = SimpleImputer(strategy=\"constant\", fill_value=self.fill_value)\n",
    "        self.imputer_2 = SimpleImputer(strategy=self.imputation_method)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        self.imputer_1.fit(X[categorical_cols])\n",
    "        self.imputer_2.fit(X[categorical_cols])\n",
    "        self.categorical_cols = categorical_cols\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # for col in self.categorical_cols:\n",
    "        #     X[f\"{col}_original\"] = X[col]\n",
    "\n",
    "        X_missing = pd.DataFrame(self.imputer_1.transform(X[self.categorical_cols]),\n",
    "                                 columns=[f\"{col}_missing\" for col in self.categorical_cols],\n",
    "                                 index=X.index)\n",
    "\n",
    "        X_frequent = pd.DataFrame(self.imputer_2.transform(X[self.categorical_cols]),\n",
    "                                  columns=[f\"{col}_frequent\" for col in self.categorical_cols],\n",
    "                                  index=X.index)\n",
    "\n",
    "        df = pd.concat([X, X_missing, X_frequent], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "class MultipleNumericalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 max_iter=10,\n",
    "                 imputation_method=\"mean\"):\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "        self.imputation_method = imputation_method\n",
    "\n",
    "        self.imputer_mean_median = mdi.MeanMedianImputer(imputation_method=imputation_method)\n",
    "        self.imputer_fill_binary = FillBinaryNumericTransformer()\n",
    "        self.imputer_bayes = IterativeImputer(\n",
    "                 estimator=BayesianRidge(),\n",
    "                 max_iter=max_iter)\n",
    "        self.imputer_knn = IterativeImputer(\n",
    "                 estimator=KNeighborsRegressor(n_neighbors=5),\n",
    "                 max_iter=max_iter)\n",
    "        self.imputer_nonLin = IterativeImputer(\n",
    "                 estimator=DecisionTreeRegressor(\n",
    "                     max_features='sqrt', random_state=0),\n",
    "                 max_iter=max_iter)\n",
    "        self.imputer_missForest = IterativeImputer(\n",
    "                 estimator=ExtraTreesRegressor(\n",
    "                     n_estimators=10, random_state=0),\n",
    "                 max_iter=max_iter)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numeric_columns = X.select_dtypes(include=['float', 'int']).columns\n",
    "        self.imputer_mean_median.fit(X[numeric_columns])\n",
    "        self.imputer_fill_binary.fit(X[numeric_columns])\n",
    "        self.imputer_bayes.fit(X[numeric_columns])\n",
    "        self.imputer_knn.fit(X[numeric_columns])\n",
    "        self.imputer_nonLin.fit(X[numeric_columns])\n",
    "        self.imputer_missForest.fit(X[numeric_columns])\n",
    "        self.numeric_columns = numeric_columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # for col in self.numeric_columns:\n",
    "        #     X[f\"{col}_original\"] = X[col]\n",
    "\n",
    "        X_mean_median = pd.DataFrame(self.imputer_mean_median.transform(X[self.numeric_columns]),\n",
    "                                    columns=[f\"{col}_mean_median\" for col in self.numeric_columns],\n",
    "                                    index=X.index)\n",
    "\n",
    "        X_fill_binary = pd.DataFrame(self.imputer_fill_binary.transform(X[self.numeric_columns]),\n",
    "                                    columns=[f\"{col}_fill_binary\" for col in self.numeric_columns],\n",
    "                                    index=X.index)\n",
    "\n",
    "        X_bayes = pd.DataFrame(self.imputer_bayes.transform(X[self.numeric_columns]),\n",
    "                            columns=[f\"{col}_bayes\" for col in self.numeric_columns],\n",
    "                            index=X.index)\n",
    "\n",
    "        X_knn = pd.DataFrame(self.imputer_knn.transform(X[self.numeric_columns]),\n",
    "                            columns=[f\"{col}_knn\" for col in self.numeric_columns],\n",
    "                            index=X.index)\n",
    "\n",
    "        X_nonLin = pd.DataFrame(self.imputer_nonLin.transform(X[self.numeric_columns]),\n",
    "                                columns=[f\"{col}_nonLin\" for col in self.numeric_columns],\n",
    "                                index=X.index)\n",
    "\n",
    "        X_missForest = pd.DataFrame(self.imputer_missForest.transform(X[self.numeric_columns]),\n",
    "                                    columns=[f\"{col}_missForest\" for col in self.numeric_columns],\n",
    "                                    index=X.index)\n",
    "\n",
    "        df = pd.concat([X, X_mean_median, X_fill_binary, X_bayes, X_knn, X_nonLin, X_missForest], axis=1)\n",
    "\n",
    "        df = df.dropna(how='any', axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "class DualCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_1 = CountFrequencyEncoder(encoding_method='frequency', missing_values='ignore')\n",
    "        self.imputer_2 = OrdinalEncoder(encoding_method='ordered', missing_values='ignore')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        X_cols = list(X.columns)\n",
    "\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.X_cols = X_cols\n",
    "\n",
    "        self.imputer_1.fit(X[categorical_cols])\n",
    "        self.imputer_2.fit(X[categorical_cols], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_1 = pd.DataFrame(\n",
    "            self.imputer_1.transform(X[self.categorical_cols]),\n",
    "            index=X.index\n",
    "        )\n",
    "        X_1.columns = [f\"{col}_count_frequency\" for col in self.categorical_cols]\n",
    "\n",
    "        X_2 = pd.DataFrame(\n",
    "            self.imputer_2.transform(X[self.categorical_cols]),\n",
    "            index=X.index\n",
    "        )\n",
    "        X_2.columns = [f\"{col}_ordinal_enc\" for col in self.categorical_cols]\n",
    "\n",
    "        df = pd.concat([X, X_1, X_2], axis=1)\n",
    "        df = df.dropna(how='any', axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "class TripleCategoricalDiscretiser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 quantiles=10,\n",
    "                 scoring='accuracy',\n",
    "                 param_grid={'max_depth': [1,2,3,4]}):\n",
    "\n",
    "        self.scoring = scoring\n",
    "        self.param_grid = param_grid\n",
    "        self.q = quantiles\n",
    "\n",
    "        self.imputer_1 = EqualFrequencyDiscretiser(q=quantiles, return_object=True)\n",
    "        self.imputer_2 = DecisionTreeDiscretiser(cv=10,\n",
    "                                                 scoring=scoring,\n",
    "                                                 regression=False,\n",
    "                                                 param_grid=param_grid)\n",
    "        self.imputer_3 = OrdinalEncoder(encoding_method='ordered', missing_values='ignore')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_equal_freq = pd.DataFrame()\n",
    "        numerical_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "        self.numerical_cols = numerical_cols\n",
    "\n",
    "        self.imputer_1.fit(X[numerical_cols])\n",
    "        self.imputer_2.fit(X[numerical_cols], y)\n",
    "\n",
    "        X_equal_freq = self.imputer_1.transform(X[numerical_cols])\n",
    "        X_equal_freq.columns = [f\"{col}_equal_freq\" for col in self.numerical_cols]\n",
    "        self.imputer_3.fit(X_equal_freq, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        X_equal_freq_new = pd.DataFrame(self.imputer_1.transform(X[self.numerical_cols]), index=X.index)\n",
    "        X_equal_freq_new.columns = [f\"{col}_equal_freq\" for col in self.numerical_cols]\n",
    "\n",
    "        X_decision_tree = pd.DataFrame(self.imputer_2.transform(X[self.numerical_cols]), index=X.index)\n",
    "        X_decision_tree.columns = [f\"{col}_decision_tree\" for col in self.numerical_cols]\n",
    "\n",
    "        X_ordinal = pd.DataFrame(self.imputer_3.transform(X_equal_freq_new), index=X.index)\n",
    "        X_ordinal.columns = [f\"{col}_ef_ordinal\" for col in X_equal_freq_new.columns]\n",
    "\n",
    "        df = pd.concat([X, X_equal_freq_new, X_decision_tree, X_ordinal], axis=1)\n",
    "        df = df.dropna(how='any', axis=1)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                pd.to_numeric(df[col])\n",
    "            except ValueError:\n",
    "                cols_to_drop.append(col)\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "        return df\n",
    "\n",
    "class DualTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.transformer_1 = fe_PowerTransformer(exp=0.333)\n",
    "        self.transformer_2 = sk_PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer_1.fit(X)\n",
    "        self.transformer_2.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_1 = pd.DataFrame(\n",
    "            self.transformer_1.transform(X),\n",
    "            index=X.index)\n",
    "        X_1.columns = [f\"{col}_cbrt\" for col in self.transformer_1.get_feature_names_out()]\n",
    "\n",
    "        X_2 = pd.DataFrame(\n",
    "            self.transformer_2.transform(X),\n",
    "            index=X.index)\n",
    "        X_2.columns = [f\"{col}_yj\" for col in self.transformer_2.get_feature_names_out()]\n",
    "\n",
    "        df = pd.concat([X, X_1, X_2], axis=1)\n",
    "        df = df.dropna(how='any', axis=1)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input methods analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('bool', BooleanTransformer(), ['VIP', 'CryoSleep']),\n",
    "        # You can add numeric or categorical steps here as well\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('add_passenger_group', AddPassengerGroup()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    (\"categorical_imputer\", DualCategoricalImputer(fill_value=\"missing\", imputation_method=\"most_frequent\")),\n",
    "    (\"numerical_imputer\", MultipleNumericalImputer(max_iter=10, imputation_method='mean')),\n",
    "\n",
    "    # Possibly other steps (scaler, model, etc.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature = \"RoomService\"\n",
    "\n",
    "original_values = np.log1p(train_data[feature].astype(float))\n",
    "nonLin = np.log1p(imputed_data[f\"{feature}_nonLin\"].astype(float))\n",
    "bayes = np.log1p(imputed_data[f\"{feature}_bayes\"].astype(float))\n",
    "knn = np.log1p(imputed_data[f\"{feature}_knn\"].astype(float))  # Fixed reference\n",
    "missForest = np.log1p(imputed_data[f\"{feature}_missForest\"].astype(float))\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "violin_parts = plt.violinplot(\n",
    "    [bayes, knn, nonLin, missForest],\n",
    "    showmeans=True,\n",
    "    widths=0.9\n",
    ")\n",
    "\n",
    "plt.xticks([1, 2, 3, 4], [\"Bayesian Ridge\", \"KNN\", \"Non-Linear\", \"MissForest\"])\n",
    "\n",
    "plt.title(f\"Comparison of Imputed Values for {feature}\")\n",
    "plt.ylabel(f\"Log({feature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare imputation methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Bayesian Ridge\": bayes,\n",
    "    \"KNN\": knn,\n",
    "    \"Non-Linear\": nonLin,\n",
    "    \"MissForest\": missForest\n",
    "})\n",
    "\n",
    "# Compute statistical properties\n",
    "stats_summary = comparison_df.describe()\n",
    "stats_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_booleans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('bool', BooleanTransformer(), ['VIP', 'CryoSleep']),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ## Basic\n",
    "    ('add_passenger_group', AddPassengerGroup()),\n",
    "    ('convert_booleans', convert_booleans),\n",
    "\n",
    "    ## Imputers\n",
    "    (\"categorical_imputer\", DualCategoricalImputer(fill_value=\"missing\", imputation_method=\"most_frequent\")),\n",
    "    (\"numerical_imputer\", MultipleNumericalImputer(max_iter=20, imputation_method='mean')),\n",
    "\n",
    "    ## Encoders\n",
    "    (\"categorical_encoder\", DualCategoricalEncoder()),\n",
    "\n",
    "    ## Discretizers\n",
    "    (\"categorical_discretiser\", TripleCategoricalDiscretiser(quantiles=10)),\n",
    "\n",
    "    ## Numerical transformations\n",
    "    (\"numerical_transformer\", DualTransformer()),\n",
    "\n",
    "    ## Math computations\n",
    "\n",
    "    ## Scaling\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_array = pipeline.fit_transform(X_train, y_train)\n",
    "imputed_df = pd.DataFrame(imputed_array, columns=pipeline[7].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.to_parquet(path / 'X_train.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).to_parquet(path / 'y_train.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=None)\n",
    "\n",
    "pca_df = pca.fit_transform(imputed_df)\n",
    "pca_df = pd.DataFrame(pca_df, columns=pca.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Percentage of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Percentage of Variance Explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(pca_df, 'pca0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
