{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"read_data\"></a> Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (Path('..') / 'Kaggle-Titanic-Spacechip-Competion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path / 'train.csv')\n",
    "test_data = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Transported']\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "train_data = train_data.drop(columns=['Transported',\n",
    "                                      'PassengerId',\n",
    "                                      'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(train_data, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The name of the passengers isn't going to be used for nothing in this notebook\n",
    "# train_data_1 = train_data.drop('Name', axis=1)\n",
    "# test_data_1 = test_data.drop('Name',axis=1)\n",
    "# # The passenger IDs will be used at the end to return the correct IDs with the model previsions\n",
    "# passenger_id = train_data_1['PassengerId']\n",
    "# test_passenger_id = test_data_1['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import feature_engine.imputation as mdi\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.discretisation import DecisionTreeDiscretiser\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "from feature_engine.transformation import PowerTransformer  as fe_PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer as sk_PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class BooleanTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return list(input_features)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['VIP'] = X['VIP'].map({True: 1, False: 0})\n",
    "        X['CryoSleep'] = X['CryoSleep'].map({True: 1, False: 0})\n",
    "        return X\n",
    "\n",
    "class AddPassengerGroup(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['passenger_group'] = X['PassengerId'].str[:4]\n",
    "        X = X.drop(columns=['PassengerId'], axis=1)\n",
    "        return X\n",
    "\n",
    "class FillBinaryNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        binary_columns = ['CryoSleep','VIP']\n",
    "        numeric_columns = ['FoodCourt','RoomService','Spa','VRDeck','ShoppingMall']\n",
    "\n",
    "        # I am assuming here that if there is no record of the person in CryoSleep or if they are on the VIP list they probably aren't in neither\n",
    "        X.loc[:, binary_columns] = X[binary_columns].fillna(False)\n",
    "        # The same logic applies here, if there is no record of the passenger spending money, they probably didn't spend it\n",
    "        X.loc[:, numeric_columns] = X[numeric_columns].fillna(0)\n",
    "        return X\n",
    "\n",
    "class FillCabinDestHomeAgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Here the function groups the data by the passanger group, and fills in the other data based on the info of another passanger that's in their group\n",
    "        # For example if the daughter didn't put her data correct but her mother did, the code will retrieve that information\n",
    "        X['Cabin'] = X.groupby('passenger_group')['Cabin'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Destination'] = X.groupby('passenger_group')['Destination'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['HomePlanet'] = X.groupby('passenger_group')['HomePlanet'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        return X\n",
    "\n",
    "class FillRestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # For the groups that didn't have any information, I just decided to fill in the Age with the mean of values from that group and the other just a 'missing' string\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X['Age'] = X['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X[['Cabin','HomePlanet','Destination']] = X[['Cabin','HomePlanet','Destination']].fillna('missing')\n",
    "        return X\n",
    "\n",
    "class DualCategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 fill_value=\"missing\",\n",
    "                 imputation_method=\"most_frequent\",\n",
    "                 imputer_choice=\"both\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        fill_value: any, default=\"missing\"\n",
    "            The constant value to use when imputing missing values.\n",
    "        imputation_method: str, default=\"most_frequent\"\n",
    "            The strategy for imputing missing values (e.g., \"most_frequent\").\n",
    "        imputer_choice: str, default=\"both\"\n",
    "            Determines which imputer to apply.\n",
    "            Options:\n",
    "                \"both\"      - apply both the constant fill imputer and the chosen strategy imputer (default)\n",
    "                \"constant\"  - apply only the constant fill imputer\n",
    "                \"frequent\"  - apply only the strategy imputer (e.g., most frequent)\n",
    "        \"\"\"\n",
    "        self.fill_value = fill_value\n",
    "        self.imputation_method = imputation_method\n",
    "        self.imputer_choice = imputer_choice\n",
    "\n",
    "        self.imputer_1 = SimpleImputer(strategy=\"constant\", fill_value=self.fill_value)\n",
    "        self.imputer_2 = SimpleImputer(strategy=self.imputation_method)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Select only the categorical columns.\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "        if self.imputer_choice in [\"both\", \"constant\"]:\n",
    "            self.imputer_1.fit(X[categorical_cols])\n",
    "        if self.imputer_choice in [\"both\", \"frequent\"]:\n",
    "            self.imputer_2.fit(X[categorical_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        df_list = [X]  # Optionally include original columns\n",
    "\n",
    "        if self.imputer_choice in [\"both\", \"constant\"]:\n",
    "            X_missing = pd.DataFrame(\n",
    "                self.imputer_1.transform(X[self.categorical_cols]),\n",
    "                columns=[f\"{col}_missing\" for col in self.categorical_cols],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_missing)\n",
    "\n",
    "        if self.imputer_choice in [\"both\", \"frequent\"]:\n",
    "            X_frequent = pd.DataFrame(\n",
    "                self.imputer_2.transform(X[self.categorical_cols]),\n",
    "                columns=[f\"{col}_frequent\" for col in self.categorical_cols],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_frequent)\n",
    "\n",
    "        df = pd.concat(df_list, axis=1)\n",
    "        return df\n",
    "\n",
    "class MultipleNumericalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 max_iter=10,\n",
    "                 imputation_method=\"mean\",\n",
    "                 imputer_choice=\"all\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_iter : int, default=10\n",
    "            Maximum number of iterations for the iterative imputers.\n",
    "        imputation_method : str, default=\"mean\"\n",
    "            The imputation method for the mean/median imputer.\n",
    "        imputer_choice : str or list, default=\"all\"\n",
    "            Specifies which imputation method(s) to apply.\n",
    "            Options (if string):\n",
    "                \"all\"         - apply all available imputers.\n",
    "                \"mean_median\" - use the mean/median imputer.\n",
    "                \"fill_binary\" - use the binary fill imputer.\n",
    "                \"bayes\"       - use the Bayesian Ridge-based imputer.\n",
    "                \"knn\"         - use the KNeighbors-based imputer.\n",
    "                \"nonlin\"      - use the Decision Tree-based imputer.\n",
    "                \"missforest\"  - use the Extra Trees-based imputer.\n",
    "            You can also pass a list of these strings to combine methods.\n",
    "        \"\"\"\n",
    "        self.max_iter = max_iter\n",
    "        self.imputation_method = imputation_method\n",
    "        self.imputer_choice = imputer_choice\n",
    "\n",
    "        self.imputer_mean_median = mdi.MeanMedianImputer(imputation_method=imputation_method)\n",
    "        self.imputer_fill_binary = FillBinaryNumericTransformer()\n",
    "        self.imputer_bayes = IterativeImputer(estimator=BayesianRidge(), max_iter=max_iter)\n",
    "        self.imputer_knn = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=5), max_iter=max_iter)\n",
    "        self.imputer_nonLin = IterativeImputer(estimator=DecisionTreeRegressor(max_features='sqrt', random_state=0), max_iter=max_iter)\n",
    "        self.imputer_missForest = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=max_iter)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numeric_columns = X.select_dtypes(include=['float', 'int']).columns\n",
    "        self.numeric_columns = numeric_columns\n",
    "\n",
    "        # Process imputer_choice into a list of choices\n",
    "        if isinstance(self.imputer_choice, str):\n",
    "            choices = [self.imputer_choice.lower()]\n",
    "        else:\n",
    "            choices = [choice.lower() for choice in self.imputer_choice]\n",
    "        if \"all\" in choices:\n",
    "            choices = [\"mean_median\", \"fill_binary\", \"bayes\", \"knn\", \"nonlin\", \"missforest\"]\n",
    "        self.chosen_imputers = choices\n",
    "\n",
    "        # Fit only the selected imputers on the numeric columns\n",
    "        if \"mean_median\" in self.chosen_imputers:\n",
    "            self.imputer_mean_median.fit(X[numeric_columns])\n",
    "        if \"fill_binary\" in self.chosen_imputers:\n",
    "            self.imputer_fill_binary.fit(X[numeric_columns])\n",
    "        if \"bayes\" in self.chosen_imputers:\n",
    "            self.imputer_bayes.fit(X[numeric_columns])\n",
    "        if \"knn\" in self.chosen_imputers:\n",
    "            self.imputer_knn.fit(X[numeric_columns])\n",
    "        if \"nonlin\" in self.chosen_imputers:\n",
    "            self.imputer_nonLin.fit(X[numeric_columns])\n",
    "        if \"missforest\" in self.chosen_imputers:\n",
    "            self.imputer_missForest.fit(X[numeric_columns])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        df_list = [X]  # Start with the original data\n",
    "\n",
    "        if \"mean_median\" in self.chosen_imputers:\n",
    "            X_mean_median = pd.DataFrame(\n",
    "                self.imputer_mean_median.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_mean_median\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_mean_median)\n",
    "\n",
    "        if \"fill_binary\" in self.chosen_imputers:\n",
    "            X_fill_binary = pd.DataFrame(\n",
    "                self.imputer_fill_binary.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_fill_binary\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_fill_binary)\n",
    "\n",
    "        if \"bayes\" in self.chosen_imputers:\n",
    "            X_bayes = pd.DataFrame(\n",
    "                self.imputer_bayes.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_bayes\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_bayes)\n",
    "\n",
    "        if \"knn\" in self.chosen_imputers:\n",
    "            X_knn = pd.DataFrame(\n",
    "                self.imputer_knn.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_knn\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_knn)\n",
    "\n",
    "        if \"nonlin\" in self.chosen_imputers:\n",
    "            X_nonLin = pd.DataFrame(\n",
    "                self.imputer_nonLin.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_nonLin\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_nonLin)\n",
    "\n",
    "        if \"missforest\" in self.chosen_imputers:\n",
    "            X_missForest = pd.DataFrame(\n",
    "                self.imputer_missForest.transform(X[self.numeric_columns]),\n",
    "                columns=[f\"{col}_missForest\" for col in self.numeric_columns],\n",
    "                index=X.index\n",
    "            )\n",
    "            df_list.append(X_missForest)\n",
    "\n",
    "        df = pd.concat(df_list, axis=1)\n",
    "\n",
    "        # Post-processing: if any column contains missing values,\n",
    "        # set non-null values to NaN and then fill with 0.\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                df.loc[df[col].notnull(), col] = np.nan\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "class DualCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer_1 = CountFrequencyEncoder(encoding_method='frequency', missing_values='ignore')\n",
    "        self.imputer_2 = OrdinalEncoder(encoding_method='ordered', missing_values='ignore')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        X_cols = list(X.columns)\n",
    "\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.X_cols = X_cols\n",
    "\n",
    "        self.imputer_1.fit(X[categorical_cols])\n",
    "        self.imputer_2.fit(X[categorical_cols], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_1 = pd.DataFrame(\n",
    "            self.imputer_1.transform(X[self.categorical_cols]),\n",
    "            index=X.index\n",
    "        )\n",
    "        X_1.columns = [f\"{col}_count_frequency\" for col in self.categorical_cols]\n",
    "\n",
    "        X_2 = pd.DataFrame(\n",
    "            self.imputer_2.transform(X[self.categorical_cols]),\n",
    "            index=X.index\n",
    "        )\n",
    "        X_2.columns = [f\"{col}_ordinal_enc\" for col in self.categorical_cols]\n",
    "\n",
    "        df = pd.concat([X, X_1, X_2], axis=1)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                df.loc[df[col].notnull(), col] = np.nan\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "class TripleCategoricalDiscretiser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 quantiles=10,\n",
    "                 scoring='accuracy',\n",
    "                 param_grid={'max_depth': [1, 2, 3, 4]},\n",
    "                 discretiser_choice=\"all\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        quantiles: int, default=10\n",
    "            Number of quantiles to use for the equal frequency discretiser.\n",
    "        scoring: str, default='accuracy'\n",
    "            The scoring metric for the decision tree discretiser.\n",
    "        param_grid: dict, default={'max_depth': [1, 2, 3, 4]}\n",
    "            Parameter grid for tuning the decision tree discretiser.\n",
    "        discretiser_choice: str, default=\"all\"\n",
    "            Determines which discretisation method(s) to apply.\n",
    "            Options:\n",
    "                \"all\"            - apply all three methods (equal frequency, decision tree, and ordinal)\n",
    "                \"equal_freq\"     - apply only the equal frequency discretisation\n",
    "                \"decision_tree\"  - apply only the decision tree discretisation\n",
    "                \"ordinal\"        - apply only the ordinal encoding (based on equal frequency output)\n",
    "        \"\"\"\n",
    "        self.scoring = scoring\n",
    "        self.param_grid = param_grid\n",
    "        self.q = quantiles\n",
    "        self.discretiser_choice = discretiser_choice\n",
    "\n",
    "        self.imputer_1 = EqualFrequencyDiscretiser(q=quantiles, return_object=True)\n",
    "        self.imputer_2 = DecisionTreeDiscretiser(cv=10,\n",
    "                                                 scoring=scoring,\n",
    "                                                 regression=False,\n",
    "                                                 param_grid=param_grid)\n",
    "        self.imputer_3 = OrdinalEncoder(encoding_method='ordered', missing_values='ignore')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Select only numerical columns\n",
    "        numerical_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "        self.numerical_cols = numerical_cols\n",
    "\n",
    "        if self.discretiser_choice in [\"all\", \"equal_freq\", \"ordinal\"]:\n",
    "            self.imputer_1.fit(X[numerical_cols])\n",
    "        if self.discretiser_choice in [\"all\", \"decision_tree\"]:\n",
    "            self.imputer_2.fit(X[numerical_cols], y)\n",
    "        if self.discretiser_choice in [\"all\", \"ordinal\"]:\n",
    "            # For ordinal encoding, we need to fit the ordinal encoder on the equal frequency output\n",
    "            X_equal_freq = self.imputer_1.transform(X[numerical_cols])\n",
    "            X_equal_freq = pd.DataFrame(X_equal_freq, index=X.index)\n",
    "            X_equal_freq.columns = [f\"{col}_equal_freq\" for col in self.numerical_cols]\n",
    "            self.imputer_3.fit(X_equal_freq, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        df_list = [X]  # Always start with the original data\n",
    "\n",
    "        if self.discretiser_choice in [\"all\", \"equal_freq\"]:\n",
    "            X_equal_freq_new = pd.DataFrame(self.imputer_1.transform(X[self.numerical_cols]), index=X.index)\n",
    "            X_equal_freq_new.columns = [f\"{col}_equal_freq\" for col in self.numerical_cols]\n",
    "            df_list.append(X_equal_freq_new)\n",
    "\n",
    "        if self.discretiser_choice in [\"all\", \"decision_tree\"]:\n",
    "            X_decision_tree = pd.DataFrame(self.imputer_2.transform(X[self.numerical_cols]), index=X.index)\n",
    "            X_decision_tree.columns = [f\"{col}_decision_tree\" for col in self.numerical_cols]\n",
    "            df_list.append(X_decision_tree)\n",
    "\n",
    "        if self.discretiser_choice in [\"all\", \"ordinal\"]:\n",
    "            # Compute equal frequency transformation to feed into the ordinal encoder\n",
    "            X_equal_freq_new = pd.DataFrame(self.imputer_1.transform(X[self.numerical_cols]), index=X.index)\n",
    "            X_equal_freq_new.columns = [f\"{col}_equal_freq\" for col in self.numerical_cols]\n",
    "            X_ordinal = pd.DataFrame(self.imputer_3.transform(X_equal_freq_new), index=X.index)\n",
    "            X_ordinal.columns = [f\"{col}_ef_ordinal\" for col in X_equal_freq_new.columns]\n",
    "            df_list.append(X_ordinal)\n",
    "\n",
    "        df = pd.concat(df_list, axis=1)\n",
    "\n",
    "        # Handle missing values: if any column contains NaNs, convert non-null entries to NaN then fill with 0.\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                df.loc[df[col].notnull(), col] = np.nan\n",
    "                df[col] = df[col].fillna(0)\n",
    "\n",
    "        # Drop columns that cannot be converted to numeric values.\n",
    "        cols_to_drop = []\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                pd.to_numeric(df[col])\n",
    "            except ValueError:\n",
    "                cols_to_drop.append(col)\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "        return df\n",
    "\n",
    "class DualTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer_choice=\"both\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        transformer_choice: str, default=\"both\"\n",
    "            Determines which transformation(s) to apply.\n",
    "            Options:\n",
    "                \"both\"  - apply both transformations (default)\n",
    "                \"cbrt\"  - apply only the cube-root based transformer\n",
    "                \"yj\"    - apply only the Yeo-Johnson transformer\n",
    "        \"\"\"\n",
    "        self.transformer_choice = transformer_choice\n",
    "        self.transformer_1 = fe_PowerTransformer(exp=0.333)\n",
    "        self.transformer_2 = sk_PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.transformer_choice in [\"both\", \"cbrt\"]:\n",
    "            self.transformer_1.fit(X)\n",
    "        if self.transformer_choice in [\"both\", \"yj\"]:\n",
    "            self.transformer_2.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df_list = []  # list to collect the dataframes\n",
    "\n",
    "        # Optionally, you could always keep the original data:\n",
    "        df_list.append(X)\n",
    "\n",
    "        if self.transformer_choice in [\"both\", \"cbrt\"]:\n",
    "            X_1 = pd.DataFrame(\n",
    "                self.transformer_1.transform(X),\n",
    "                index=X.index)\n",
    "            X_1.columns = [f\"{col}_cbrt\" for col in self.transformer_1.get_feature_names_out()]\n",
    "            df_list.append(X_1)\n",
    "\n",
    "        if self.transformer_choice in [\"both\", \"yj\"]:\n",
    "            X_2 = pd.DataFrame(\n",
    "                self.transformer_2.transform(X),\n",
    "                index=X.index)\n",
    "            X_2.columns = [f\"{col}_yj\" for col in self.transformer_2.get_feature_names_out()]\n",
    "            df_list.append(X_2)\n",
    "\n",
    "        return pd.concat(df_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).to_parquet(path / 'y_train.pq')\n",
    "pd.DataFrame(y_test).to_parquet(path / 'y_test.pq')\n",
    "pd.DataFrame(y_val).to_parquet(path / 'y_val.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input methods analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('bool', BooleanTransformer(), ['VIP', 'CryoSleep']),\n",
    "        # You can add numeric or categorical steps here as well\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    #('add_passenger_group', AddPassengerGroup()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    (\"categorical_imputer\", DualCategoricalImputer(fill_value=\"missing\", imputation_method=\"most_frequent\")),\n",
    "    (\"numerical_imputer\", MultipleNumericalImputer(max_iter=10, imputation_method='mean')),\n",
    "\n",
    "    # Possibly other steps (scaler, model, etc.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature = \"RoomService\"\n",
    "\n",
    "original_values = np.log1p(train_data[feature].astype(float))\n",
    "nonLin = np.log1p(imputed_data[f\"{feature}_nonLin\"].astype(float))\n",
    "bayes = np.log1p(imputed_data[f\"{feature}_bayes\"].astype(float))\n",
    "knn = np.log1p(imputed_data[f\"{feature}_knn\"].astype(float))  # Fixed reference\n",
    "missForest = np.log1p(imputed_data[f\"{feature}_missForest\"].astype(float))\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "violin_parts = plt.violinplot(\n",
    "    [bayes, knn, nonLin, missForest],\n",
    "    showmeans=True,\n",
    "    widths=0.9\n",
    ")\n",
    "\n",
    "plt.xticks([1, 2, 3, 4], [\"Bayesian Ridge\", \"KNN\", \"Non-Linear\", \"MissForest\"])\n",
    "\n",
    "plt.title(f\"Comparison of Imputed Values for {feature}\")\n",
    "plt.ylabel(f\"Log({feature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare imputation methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Bayesian Ridge\": bayes,\n",
    "    \"KNN\": knn,\n",
    "    \"Non-Linear\": nonLin,\n",
    "    \"MissForest\": missForest\n",
    "})\n",
    "\n",
    "# Compute statistical properties\n",
    "stats_summary = comparison_df.describe()\n",
    "stats_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_booleans = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('bool', BooleanTransformer(), ['VIP', 'CryoSleep']),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ## Basic\n",
    "    #('add_passenger_group', AddPassengerGroup()),\n",
    "    ('convert_booleans', convert_booleans),\n",
    "\n",
    "    ## Imputers\n",
    "    (\"categorical_imputer\", DualCategoricalImputer(imputer_choice=\"frequent\",\n",
    "                                                   fill_value=\"missing\",\n",
    "                                                   imputation_method=\"most_frequent\")),\n",
    "\n",
    "    (\"numerical_imputer\", MultipleNumericalImputer(max_iter=20,\n",
    "                                                   imputation_method='mean',\n",
    "                                                   imputer_choice=[\"bayes\", \"knn\"])),\n",
    "\n",
    "    ## Encoders\n",
    "    (\"categorical_encoder\", DualCategoricalEncoder()),\n",
    "\n",
    "    ## Discretizers\n",
    "    (\"categorical_discretiser\", TripleCategoricalDiscretiser(discretiser_choice=\"decision_tree\",\n",
    "                                                             quantiles=10)),\n",
    "\n",
    "    ## Numerical transformations\n",
    "    (\"numerical_transformer\", DualTransformer(transformer_choice=\"cbrt\")),\n",
    "    ## Math computations\n",
    "\n",
    "    ## Scaling\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    ('quantile', QuantileTransformer(output_distribution='normal'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_array = pipeline.fit_transform(X_train, y_train)\n",
    "X_train = pd.DataFrame(imputed_train_array, columns=pipeline[6].get_feature_names_out())\n",
    "\n",
    "imputed_train_array = pipeline.transform(X_test)\n",
    "X_test = pd.DataFrame(imputed_train_array, columns=pipeline[6].get_feature_names_out())\n",
    "\n",
    "imputed_train_array = pipeline.transform(X_val)\n",
    "X_val = pd.DataFrame(imputed_train_array, columns=pipeline[6].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_parquet(path / 'X_train.pq')\n",
    "X_test.to_parquet(path / 'X_test.pq')\n",
    "X_val.to_parquet(path / 'X_val.pq')\n",
    "\n",
    "pd.DataFrame(y_train).to_parquet(path / 'y_train.pq')\n",
    "pd.DataFrame(y_test).to_parquet(path / 'y_test.pq')\n",
    "pd.DataFrame(y_val).to_parquet(path / 'y_val.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Impute missing values (e.g., using mean)\n",
    "imputer = SimpleImputer(strategy=\"mean\")  # Other options: \"median\", \"most_frequent\", etc.\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.85)\n",
    "pca_train = pca.fit_transform(X_train_imputed)\n",
    "pca_test = pca.transform(X_test_imputed)\n",
    "pca_val = pca.transform(X_val_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Percentage of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Percentage of Variance Explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(pd.DataFrame(pca_train), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_train).iloc[:, :11].to_parquet(path / 'pca_train.pq')\n",
    "pd.DataFrame(pca_test).iloc[:, :11].to_parquet(path / 'pca_test.pq')\n",
    "pd.DataFrame(pca_val).iloc[:, :11].to_parquet(path / 'pca_val.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
