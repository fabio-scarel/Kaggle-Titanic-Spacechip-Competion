{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07caa507",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef99685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import joblib\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox, randint, uniform, reciprocal, expon\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import (train_test_split, StratifiedShuffleSplit,\n",
    "                                     GridSearchCV, cross_val_score, RandomizedSearchCV)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (FunctionTransformer, LabelEncoder, MinMaxScaler,\n",
    "                                   Normalizer, OneHotEncoder, OrdinalEncoder, PolynomialFeatures,\n",
    "                                   PowerTransformer, QuantileTransformer, StandardScaler)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbf659",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (Path('..') / 'Kaggle-Titanic-Spacechip-Competion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path / 'train.csv')\n",
    "test_data = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcb85f",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcd98d",
   "metadata": {},
   "source": [
    "### Tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8817be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the passengers isn't going to be used for nothing in this notebook\n",
    "test_data.drop('Name',inplace=True,axis=1)\n",
    "train_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# The passenger group was utilized in processing steps so I already added it here to the DataFrame\n",
    "test_data['passenger_group']=test_data['PassengerId'].apply(lambda x: x[0:4])\n",
    "train_data['passenger_group']=train_data['PassengerId'].apply(lambda x: x[0:4])\n",
    "\n",
    "# The passenger IDs will be used at the end to return the correct IDs with the model previsions\n",
    "test_passenger_id = test_data['PassengerId']\n",
    "passenger_id = train_data['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ef397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a40c",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e305a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['CryoSleep','VIP']\n",
    "numeric_columns = ['FoodCourt','RoomService','Spa','VRDeck','ShoppingMall']\n",
    "\n",
    "cabin_ix = 3 # The cabin index is used in the CabinSeparator class\n",
    "\n",
    "class CabinSeparator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # The Cabin info has the aggregated information of the cabin deck, side and number. This function stores all the information but just returns to the original\n",
    "    # dataframe the cabin deck and side, given that the number can go until past 1000 and would probably overfit the data and not help very much\n",
    "    def __init__(self, add_separate_cabin=True):\n",
    "        self.add_separate_cabin = add_separate_cabin\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if self.add_separate_cabin:\n",
    "            X = pd.DataFrame(X)\n",
    "            cabin_deck = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0])\n",
    "            cabin_num = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0] if len(str(s).split('/'))==1 else str(s).split('/')[1])\n",
    "            cabin_side = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0] if len(str(s).split('/'))==1 else str(s).split('/')[2])\n",
    "            X['cabin_deck'] = cabin_deck\n",
    "            X['cabin_side'] = cabin_side\n",
    "            X = X.drop(X.columns[[cabin_ix]], axis=1)\n",
    "            return X.values\n",
    "        else:\n",
    "            return X\n",
    "    def get_feature_names_out(self):\n",
    "        return ['cabin_deck','cabin_num','cabin_side']\n",
    "\n",
    "\n",
    "class FillBinaryNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # I am assuming here that if there is no record of the person in CryoSleep or if they are on the VIP list they probably aren't in neither\n",
    "        X.loc[:, binary_columns] = X[binary_columns].fillna(False)\n",
    "        # The same logic applies here, if there is no record of the passenger spending money, they probably didn't spend it\n",
    "        X.loc[:, numeric_columns] = X[numeric_columns].fillna(0)\n",
    "        return X\n",
    "\n",
    "class FillCabinDestHomeAgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Here the function groups the data by the passanger group, and fills in the other data based on the info of another passanger that's in their group\n",
    "        # For example if the daughter didn't put her data correct but her mother did, the code will retrieve that information\n",
    "        X['Cabin'] = X.groupby('passenger_group')['Cabin'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Destination'] = X.groupby('passenger_group')['Destination'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['HomePlanet'] = X.groupby('passenger_group')['HomePlanet'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        return X\n",
    "\n",
    "class FillRestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # For the groups that didn't have any information, I just decided to fill in the Age with the mean of values from that group and the other just a 'None' string\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X['Age'] = X['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X[['Cabin','HomePlanet','Destination']] = X[['Cabin','HomePlanet','Destination']].fillna('None')\n",
    "        return X\n",
    "\n",
    "class AddTotalSpent(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        total_spent = np.sum(X[:, 5:10], axis=1)\n",
    "        X = np.column_stack((X, total_spent))\n",
    "        return X\n",
    "\n",
    "class AddPolyFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=3, addpoly=True):\n",
    "        self.degree = degree\n",
    "        self.addpoly = addpoly\n",
    "        self.poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.addpoly:\n",
    "            self.poly.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.addpoly:\n",
    "            return self.poly.transform(X)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Ensure feature names are passed through correctly\"\"\"\n",
    "        if self.addpoly:\n",
    "            return self.poly.get_feature_names_out(input_features)\n",
    "        return np.array(input_features) if input_features is not None else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe640b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_column_names = [col for col in train_data.columns if col != 'Cabin'] + ['cabin_deck','cabin_side','total_spent']\n",
    "test_column_names = [col for col in test_data.columns if col != 'Cabin'] + ['cabin_deck','cabin_side','total_spent']\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    ('binary_numeric', FillBinaryNumericTransformer()),\n",
    "    ('cabin_dest_home_age', FillCabinDestHomeAgeTransformer()),\n",
    "    ('rest', FillRestTransformer()),\n",
    "    ('cabin_separator', CabinSeparator()),\n",
    "    ('add_total_spent', AddTotalSpent()),\n",
    "])\n",
    "\n",
    "treated_train_data = pd.DataFrame(preprocessing.fit_transform(train_data), columns=train_column_names)\n",
    "treated_test_data = pd.DataFrame(preprocessing.transform(test_data), columns=test_column_names)\n",
    "\n",
    "columns_to_convert = ['total_spent', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Convert columns to numeric type\n",
    "treated_train_data[columns_to_convert] = treated_train_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "treated_test_data[columns_to_convert] = treated_test_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "treated_train_data[['VIP','CryoSleep']] = treated_train_data[['VIP','CryoSleep']].astype(int)\n",
    "treated_test_data[['VIP','CryoSleep']] = treated_test_data[['VIP','CryoSleep']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0b0ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = treated_train_data[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','total_spent']]\n",
    "numeric_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76721d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_data_ = treated_train_data.drop(['PassengerId','passenger_group'], axis=1)\n",
    "treated_test_data = treated_test_data.drop(['PassengerId','passenger_group'], axis=1)\n",
    "\n",
    "y_train = treated_train_data_['Transported']\n",
    "\n",
    "treated_train_data_dropped = treated_train_data_.drop(['Transported'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(treated_train_data_dropped, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37766784",
   "metadata": {},
   "source": [
    "def create_inverse(X):\n",
    "\n",
    "    X['1/Age'] = 1/(X['Age']+0.001)\n",
    "    X['1/RoomService'] = 1/(X['RoomService']+0.001)\n",
    "    X['1/FoodCourt'] = 1/(X['FoodCourt']+0.001)\n",
    "    X['1/ShoppingMall'] = 1/(X['ShoppingMall']+0.001)\n",
    "    X['1/Spa'] = 1/(X['Spa']+0.001)\n",
    "    X['1/VRDeck'] = 1/(X['VRDeck']+0.001)\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = create_inverse(X_train)\n",
    "X_val = create_inverse(X_val)\n",
    "X_test = create_inverse(X_test)\n",
    "treated_test_data = create_inverse(treated_test_data)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbed699",
   "metadata": {},
   "source": [
    "### Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38138b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PCA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad146388",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing pipelines for numeric and categorical data\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('pca', PCA(n_components=0.865)),\n",
    "    ('add_poly', AddPolyFeatures()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# Combine numeric and categorical pipelines using make_column_selector\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, make_column_selector(dtype_include=['int64', 'float64'])),\n",
    "        ('cat', categorical_pipeline, make_column_selector(dtype_include='object'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the final pipeline including the preprocessor and the quantile transformer\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('quantile', QuantileTransformer(output_distribution='normal'))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "final_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c49af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_feat = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'total_spent']\n",
    "\n",
    "other_feat_pipeline = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('quantile', QuantileTransformer(output_distribution='normal'))\n",
    "])\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "other_feat_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale_quantile', other_feat_pipeline, other_feat)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the remaining columns unchanged\n",
    ")\n",
    "\n",
    "# Define the final pipeline\n",
    "other_feat_final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', other_feat_preprocessor)\n",
    "])\n",
    "\n",
    "other_feat_final_pipeline.fit(X_train[other_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bcc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423cf6d",
   "metadata": {},
   "source": [
    "### Apply the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = final_pipeline.transform(X_train)\n",
    "\n",
    "X_val_transformed = final_pipeline.transform(X_val)\n",
    "\n",
    "X_test_transformed = final_pipeline.transform(X_test)\n",
    "\n",
    "transformed_test_data = final_pipeline.transform(treated_test_data)\n",
    "\n",
    "print(\"Transformed shape:\", X_train_transformed.shape)\n",
    "print(\"Feature names shape:\", len(final_pipeline.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_treated = pd.DataFrame(X_train_transformed, columns=final_pipeline.get_feature_names_out())\n",
    "\n",
    "X_val_treated = pd.DataFrame(X_val_transformed, columns=final_pipeline.get_feature_names_out())\n",
    "\n",
    "X_test_treated = pd.DataFrame(X_test_transformed, columns=final_pipeline.get_feature_names_out())\n",
    "\n",
    "test_data = pd.DataFrame(transformed_test_data, columns=final_pipeline.get_feature_names_out())\n",
    "\n",
    "other_X_train = pd.DataFrame(other_feat_final_pipeline.transform(X_train[other_feat]), columns=other_feat_final_pipeline.get_feature_names_out())\n",
    "\n",
    "other_X_val = pd.DataFrame(other_feat_final_pipeline.transform(X_val[other_feat]), columns=other_feat_final_pipeline.get_feature_names_out())\n",
    "\n",
    "other_X_test = pd.DataFrame(other_feat_final_pipeline.transform(X_test[other_feat]), columns=other_feat_final_pipeline.get_feature_names_out())\n",
    "\n",
    "other_test_data = pd.DataFrame(other_feat_final_pipeline.transform(treated_test_data[other_feat]), columns=other_feat_final_pipeline.get_feature_names_out())\n",
    "\n",
    "X_train = pd.concat([X_train_treated, other_X_train], axis=1)\n",
    "X_val = pd.concat([X_val_treated, other_X_val], axis=1)\n",
    "X_test = pd.concat([X_test_treated, other_X_test], axis=1)\n",
    "final_test = pd.concat([test_data, other_test_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd42c2c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a71e63",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f571e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 6))  # Create a figure and a 1x2 subplot\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0][0].scatter(treated_train_data['Transported'], treated_train_data['VRDeck'])\n",
    "axs[0][0].set_xlabel('Transported')\n",
    "axs[0][0].set_ylabel('VRDeck')\n",
    "axs[0][0].set_title('Scatter Plot: Transported vs VRDeck')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[0][1].scatter(treated_train_data['Transported'], treated_train_data['ShoppingMall'])\n",
    "axs[0][1].set_xlabel('Transported')\n",
    "axs[0][1].set_ylabel('ShoppingMall')\n",
    "axs[0][1].set_title('Scatter Plot: Transported vs ShoppingMall')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1][0].scatter(treated_train_data['Transported'], treated_train_data['Spa'])\n",
    "axs[1][0].set_xlabel('Transported')\n",
    "axs[1][0].set_ylabel('Spa')\n",
    "axs[1][0].set_title('Scatter Plot: Transported vs Spa')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1][1].scatter(treated_train_data['Transported'], treated_train_data['RoomService'])\n",
    "axs[1][1].set_xlabel('Transported')\n",
    "axs[1][1].set_ylabel('RoomService')\n",
    "axs[1][1].set_title('Scatter Plot: Transported vs RoomService')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[2][0].scatter(treated_train_data['Transported'], treated_train_data['total_spent'])\n",
    "axs[2][0].set_xlabel('Transported')\n",
    "axs[2][0].set_ylabel('total_spent')\n",
    "axs[2][0].set_title('Scatter Plot: Transported vs Total Spent')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[2][1].scatter(treated_train_data['Transported'], treated_train_data['FoodCourt'])\n",
    "axs[2][1].set_xlabel('Transported')\n",
    "axs[2][1].set_ylabel('FoodCourt')\n",
    "axs[2][1].set_title('Scatter Plot: Transported vs FoodCourt')\n",
    "\n",
    "plt.tight_layout()  # Adjust the padding between and around the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ix_food = treated_train_data[(treated_train_data['FoodCourt']>20000)].index\n",
    "drop_ix_shopping = treated_train_data[(treated_train_data['ShoppingMall']>10000)].index\n",
    "\n",
    "treated_train_data = treated_train_data.drop(drop_ix_food)\n",
    "treated_train_data = treated_train_data.drop(drop_ix_shopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f874a",
   "metadata": {},
   "source": [
    "### Checking relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc625d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "figsize = (1.5, 1.3)\n",
    "g = sns.PairGrid(train_data, hue='Transported', vars=numerical_columns, height=figsize[1], aspect=figsize[0]/figsize[1])\n",
    "g.map_lower(sns.scatterplot)\n",
    "legend = g.add_legend(loc='upper right', bbox_to_anchor=(0.65, 0.7))\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 0)):\n",
    "    g.axes[i, j].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = treated_train_data['Transported'].astype(int)\n",
    "treated_train_data.drop('Transported', axis=1, inplace=True)\n",
    "treated_train_data.drop('PassengerId', axis=1, inplace=True)\n",
    "treated_train_data.drop('passenger_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7693a4",
   "metadata": {},
   "source": [
    "### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_columns), figsize=(12, 4))\n",
    "\n",
    "for i, var in enumerate(numerical_columns):\n",
    "    axes[i].hist(treated_train_data[var], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "normalizer_pipeline = Pipeline([\n",
    "    ('feature_processing', ColumnTransformer([\n",
    "        ('normalizer', PowerTransformer(method='yeo-johnson'), make_column_selector(dtype_include=float)),\n",
    "        ('encoder', OneHotEncoder(), make_column_selector(dtype_exclude=np.number))\n",
    "    ], remainder='passthrough')),\n",
    "])\n",
    "\n",
    "treated_train_data = pd.DataFrame(normalizer_pipeline.fit_transform(treated_train_data), \n",
    "                                  columns= [col.replace('normalizer__', '').replace('remainder__', '').replace('encoder__', '') for col in normalizer_pipeline.named_steps['feature_processing'].get_feature_names_out()])\n",
    "\n",
    "treated_test_data = pd.DataFrame(normalizer_pipeline.transform(treated_test_data), \n",
    "                                 columns= [col.replace('normalizer__', '').replace('remainder__', '').replace('encoder__', '') for col in normalizer_pipeline.named_steps['feature_processing'].get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670915c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_columns), figsize=(12, 4))\n",
    "\n",
    "for i, var in enumerate(numerical_columns):\n",
    "    axes[i].hist(treated_train_data[var], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d099006",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62c70f",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e56ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model = joblib.load('best_gbc_model.pkl')\n",
    "lr_model = joblib.load('best_lr_model.pkl')\n",
    "svc_model = joblib.load('best_svc_model.pkl')\n",
    "xgb_model = joblib.load('best_xgb_model.pkl')\n",
    "stacking_clf = joblib.load('stacking_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c653a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a97356",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f04832",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(model, X_val, y_val):\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    eval_accuracy = accuracy_score(y_val, y_pred, )\n",
    "\n",
    "    return eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(stacking_clf, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e103f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(gbc_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84adac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(xgb_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(lr_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(svc_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc811be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf = StackingClassifier(\n",
    "\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lr', lr_model),\n",
    "        ('svc', svc_model),\n",
    "        ('gbc', gbc_model)\n",
    "    ],\n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "accuracy_check(stacking_clf, X_val, y_val)\n",
    "#joblib.dump(stacking_clf, 'stacking_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c950825",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stacking_clf.predict(final_test).astype(bool),test_passenger_id, columns=['Transported']).to_csv(path / 'pred_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ebfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacking_clf.predict(final_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indices = np.where(y_val != stacking_clf.predict(X_val))[0]\n",
    "misclassified_samples = treated_train_data.iloc[misclassified_indices]\n",
    "misclassified_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples.VIP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples.Destination.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples.HomePlanet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8c358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
