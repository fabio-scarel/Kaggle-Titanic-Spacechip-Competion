{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import joblib\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import (train_test_split, StratifiedShuffleSplit,\n",
    "                                     GridSearchCV, cross_val_score, RandomizedSearchCV)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (FunctionTransformer, LabelEncoder, MinMaxScaler,\n",
    "                                   Normalizer, OneHotEncoder, OrdinalEncoder, PolynomialFeatures,\n",
    "                                   PowerTransformer, QuantileTransformer, StandardScaler)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (Path('..') / 'Kaggle-Titanic-Spacechip-Competion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full = pd.read_csv(path / 'train.csv')\n",
    "test_data = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the passengers isn't going to be used for nothing in this notebook\n",
    "test_data = test_data.drop('Name', axis=1)\n",
    "train_data_full = train_data_full.drop('Name', axis=1)\n",
    "\n",
    "# The passenger group was utilized in processing steps so I already added it here to the DataFrame\n",
    "test_data['passenger_group']=test_data['PassengerId'].apply(lambda x: x[0:4])\n",
    "train_data_full['passenger_group']=train_data_full['PassengerId'].apply(lambda x: x[0:4])\n",
    "\n",
    "# The passenger IDs will be used at the end to return the correct IDs with the model previsions\n",
    "test_passenger_id = test_data['PassengerId']\n",
    "passenger_id = train_data_full['PassengerId']\n",
    "\n",
    "train_data = train_data_full.drop(['Transported'], axis=1)\n",
    "y_train = train_data_full['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = ['CryoSleep','VIP']\n",
    "numeric_columns = ['FoodCourt','RoomService','Spa','VRDeck','ShoppingMall']\n",
    "\n",
    "cabin_ix = 3 # The cabin index is used in the CabinSeparator class\n",
    "\n",
    "class CabinSeparator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # The Cabin info has the aggregated information of the cabin deck, side and number. This function stores all the information but just returns to the original\n",
    "    # dataframe the cabin deck and side, given that the number can go until past 1000 and would probably overfit the data and not help very much\n",
    "    def __init__(self, add_separate_cabin=True):\n",
    "        self.add_separate_cabin = add_separate_cabin\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if self.add_separate_cabin:\n",
    "            X = pd.DataFrame(X)\n",
    "            cabin_deck = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0])\n",
    "            cabin_num = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0] if len(str(s).split('/'))==1 else str(s).split('/')[1])\n",
    "            cabin_side = X.iloc[:, cabin_ix].apply(lambda s: str(s).split('/')[0] if len(str(s).split('/'))==1 else str(s).split('/')[2])\n",
    "            X['cabin_deck'] = cabin_deck\n",
    "            X['cabin_side'] = cabin_side\n",
    "            X = X.drop(X.columns[[cabin_ix]], axis=1)\n",
    "            return X.values\n",
    "        else:\n",
    "            return X\n",
    "    def get_feature_names_out(X, self):\n",
    "        # This is wrong?\n",
    "        return list(X.columns).extend(['cabin_deck','cabin_side'])\n",
    "\n",
    "class FillBinaryNumericTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # I am assuming here that if there is no record of the person in CryoSleep or if they are on the VIP list they probably aren't in neither\n",
    "        X.loc[:, binary_columns] = X[binary_columns].fillna(False)\n",
    "        # The same logic applies here, if there is no record of the passenger spending money, they probably didn't spend it\n",
    "        X.loc[:, numeric_columns] = X[numeric_columns].fillna(0)\n",
    "        return X\n",
    "\n",
    "class FillCabinDestHomeAgeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Here the function groups the data by the passanger group, and fills in the other data based on the info of another passanger that's in their group\n",
    "        # For example if the daughter didn't put her data correct but her mother did, the code will retrieve that information\n",
    "        X['Cabin'] = X.groupby('passenger_group')['Cabin'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Destination'] = X.groupby('passenger_group')['Destination'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['HomePlanet'] = X.groupby('passenger_group')['HomePlanet'].transform(lambda x: x.fillna(x.iloc[0]))\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        return X\n",
    "\n",
    "class FillRestTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # For the groups that didn't have any information, I just decided to fill in the Age with the mean of values from that group and the other just a 'None' string\n",
    "        X['Age'] = X.groupby('passenger_group')['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X['Age'] = X['Age'].transform(lambda value: value.fillna(value.mean()))\n",
    "        X[['Cabin','HomePlanet','Destination']] = X[['Cabin','HomePlanet','Destination']].fillna('None')\n",
    "        return X\n",
    "\n",
    "class AddTotalSpent(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        total_spent = np.sum(X[:, 5:10], axis=1)\n",
    "        X = np.column_stack((X, total_spent))\n",
    "        return X\n",
    "\n",
    "class AddPolyFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=3, addpoly=True):\n",
    "        self.degree = degree\n",
    "        self.addpoly = addpoly\n",
    "        self.poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.addpoly:\n",
    "            self.poly.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.addpoly:\n",
    "            return self.poly.transform(X)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Ensure feature names are passed through correctly\"\"\"\n",
    "        if self.addpoly:\n",
    "            return self.poly.get_feature_names_out(input_features)\n",
    "        return np.array(input_features) if input_features is not None else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_column_names = [col for col in train_data.columns if col != 'Cabin'] + ['cabin_deck','cabin_side','total_spent']\n",
    "test_column_names = [col for col in test_data.columns if col != 'Cabin'] + ['cabin_deck','cabin_side','total_spent']\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    ('binary_numeric', FillBinaryNumericTransformer()),\n",
    "    ('cabin_dest_home_age', FillCabinDestHomeAgeTransformer()),\n",
    "    ('rest', FillRestTransformer()),\n",
    "    ('cabin_separator', CabinSeparator()),\n",
    "    ('add_total_spent', AddTotalSpent()),\n",
    "])\n",
    "\n",
    "# Maybe I should fit and transform after doing the train_test_split but I think that fitting on the train\n",
    "# and transforming on the test (that will be used to evaluate the model on Kaggle) is good enough for now\n",
    "treated_train_data = pd.DataFrame(preprocessing.fit_transform(train_data), columns=train_column_names)\n",
    "treated_test_data = pd.DataFrame(preprocessing.transform(test_data), columns=test_column_names)\n",
    "\n",
    "columns_to_convert = ['total_spent', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Convert columns to numeric type\n",
    "treated_train_data[columns_to_convert] = treated_train_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "treated_test_data[columns_to_convert] = treated_test_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "treated_train_data[['VIP','CryoSleep']] = treated_train_data[['VIP','CryoSleep']].astype(int)\n",
    "treated_test_data[['VIP','CryoSleep']] = treated_test_data[['VIP','CryoSleep']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(treated_train_data, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppose 'df' is your full training DataFrame that includes:\n",
    "# - 'passenger_group'\n",
    "# - 'cabin_deck'\n",
    "# - 'Age'\n",
    "# (You can replace 'Age' with any other numeric feature of interest.)\n",
    "\n",
    "column = 'ShoppingMall'\n",
    "\n",
    "# 1. Compute the mean Age per passenger group.\n",
    "passenger_stats = (\n",
    "    treated_train_data.groupby('passenger_group')[column]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .rename(columns={column: f'{column}_passenger_mean'})\n",
    ")\n",
    "\n",
    "# 2. Get the cabin deck for each passenger group.\n",
    "#    Assuming each passenger group is associated with one cabin deck:\n",
    "group_deck = treated_train_data[['passenger_group', 'cabin_deck']].drop_duplicates()\n",
    "\n",
    "# Merge the deck info into the passenger_stats DataFrame.\n",
    "passenger_stats = passenger_stats.merge(group_deck, on='passenger_group', how='left')\n",
    "\n",
    "# 3. Compute the mean Age per cabin deck.\n",
    "deck_stats = (\n",
    "    treated_train_data.groupby('cabin_deck')[column]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .rename(columns={column: f'{column}_deck_mean'})\n",
    ")\n",
    "\n",
    "# Merge the cabin deck stats into the passenger_stats DataFrame.\n",
    "passenger_stats = passenger_stats.merge(deck_stats, on='cabin_deck', how='left')\n",
    "\n",
    "# 4. Create a scatter plot comparing the two means.\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=passenger_stats,\n",
    "    x=f'{column}_deck_mean',\n",
    "    y=f'{column}_passenger_mean',\n",
    "    hue='cabin_deck',\n",
    "    s=100,\n",
    "    palette=\"deep\"\n",
    ")\n",
    "\n",
    "# Plot the y=x line to serve as a reference.\n",
    "min_val = min(passenger_stats[f'{column}_deck_mean'].min(), passenger_stats[f'{column}_passenger_mean'].min())\n",
    "max_val = max(passenger_stats[f'{column}_deck_mean'].max(), passenger_stats[f'{column}_passenger_mean'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='y = x')\n",
    "\n",
    "plt.xlabel(f'Cabin Deck Mean {column}')\n",
    "plt.ylabel(f'Passenger Group Mean {column}')\n",
    "plt.title(f'Comparing Cabin Deck vs. Passenger Group Mean {column}')\n",
    "plt.legend(title='Cabin Deck')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_cols = ['RoomService', 'VRDeck', 'ShoppingMall', 'FoodCourt', 'Spa', 'total_spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupStatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Computes group-level statistics on training data and merges these features onto new data.\n",
    "    The transformer computes, for each grouping variable, the following aggregations:\n",
    "      - 'Age', 'VIP', 'CryoSleep': mean\n",
    "      - 'RoomService', 'VRDeck', 'ShoppingMall', 'FoodCourt': sum\n",
    "\n",
    "    The computed statistics are renamed to include a suffix indicating the group source.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        self.base_agg_dict_ = {\n",
    "            'Age': 'mean',\n",
    "            'VIP': 'mean',\n",
    "            'CryoSleep': 'mean',\n",
    "            'RoomService': 'sum',\n",
    "            'VRDeck': 'sum',\n",
    "            'ShoppingMall': 'sum',\n",
    "            'FoodCourt': 'sum',\n",
    "            'Spa': 'sum',\n",
    "            'total_spent': 'sum'\n",
    "        }\n",
    "\n",
    "        # passenger_group\n",
    "        self.passenger_stats_ = (\n",
    "            df.groupby('passenger_group')\n",
    "              .agg(self.base_agg_dict_)\n",
    "              .reset_index()\n",
    "        )\n",
    "        self.passenger_stats_.rename(\n",
    "            columns={col: f\"{col}_gm_pass_group\" for col in self.base_agg_dict_.keys()},\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # cabin_deck\n",
    "        self.deck_stats_ = (\n",
    "            df.groupby('cabin_deck')\n",
    "              .agg(self.base_agg_dict_)\n",
    "              .reset_index()\n",
    "        )\n",
    "        self.deck_stats_.rename(\n",
    "            columns={col: f\"{col}_gm_cab_deck\" for col in self.base_agg_dict_.keys()},\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # cabin_side\n",
    "        self.side_stats_ = (\n",
    "            df.groupby('cabin_side')\n",
    "              .agg(self.base_agg_dict_)\n",
    "              .reset_index()\n",
    "        )\n",
    "        self.side_stats_.rename(\n",
    "            columns={col: f\"{col}_gm_cab_side\" for col in self.base_agg_dict_.keys()},\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        if 'passenger_group' in df.columns:\n",
    "            df = df.merge(self.passenger_stats_, on='passenger_group', how='left')\n",
    "\n",
    "        if 'cabin_deck' in df.columns:\n",
    "            df = df.merge(self.deck_stats_, on='cabin_deck', how='left')\n",
    "\n",
    "        if 'cabin_side' in df.columns:\n",
    "            df = df.merge(self.side_stats_, on='cabin_side', how='left')\n",
    "\n",
    "        df.index = X.index\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpendingClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, spending_cols, n_clusters=100, random_state=42):\n",
    "        self.spending_cols = spending_cols\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.scaler_ = StandardScaler()\n",
    "        self.scaler_.fit(X[self.spending_cols])\n",
    "        X_scaled = self.scaler_.transform(X[self.spending_cols])\n",
    "\n",
    "        # Fit KMeans clustering on the scaled spending data\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X_scaled)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X_scaled = self.scaler_.transform(X[self.spending_cols])\n",
    "        clusters = self.kmeans_.predict(X_scaled)\n",
    "        X['spending_cluster'] = clusters\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_cols = ['RoomService', 'VRDeck', 'ShoppingMall', 'FoodCourt', 'Spa', 'total_spent']\n",
    "\n",
    "group_stats_pipeline = Pipeline([\n",
    "    ('group_stats', GroupStatsTransformer()),\n",
    "    ('spending_cluster', SpendingClusterTransformer(spending_cols=spending_cols, n_clusters=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stats = group_stats_pipeline.fit_transform(X_train)\n",
    "X_test_stats = group_stats_pipeline.transform(X_test)\n",
    "X_val_stats = group_stats_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I could do some kind of otimization to reach better values for these, for example considering the total_spent amound and how much it affects it\n",
    "## but for now ill leave like this and test out the model\n",
    "\n",
    "class GroupStatsFiller(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fills missing values in the training data using group-level statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        self.base_agg_dict_ = {\n",
    "            'Age_gm_pass_group': 'mean',\n",
    "            'VIP_gm_pass_group': 'mean',\n",
    "            'CryoSleep_gm_pass_group': 'mean',\n",
    "            'RoomService_gm_pass_group': 'mean',\n",
    "            'VRDeck_gm_pass_group': 'mean',\n",
    "            'ShoppingMall_gm_pass_group': 'mean',\n",
    "            'FoodCourt_gm_pass_group': 'mean',\n",
    "            'Spa_gm_pass_group': 'mean',\n",
    "            'total_spent_gm_pass_group': 'mean'\n",
    "        }\n",
    "\n",
    "        # passenger_group\n",
    "        self.passenger_stats_ = (\n",
    "            df.groupby('spending_cluster')\n",
    "              .agg(self.base_agg_dict_)\n",
    "              .reset_index()\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        if 'spending_cluster' in df.columns:\n",
    "            df = df.merge(self.passenger_stats_, on='spending_cluster', how='left', suffixes=('', '_fill'))\n",
    "            for col in self.base_agg_dict_.keys():\n",
    "                df[col] = df[col].fillna(df[f'{col}_fill'])\n",
    "                df = df.drop(columns=[f'{col}_fill'])\n",
    "\n",
    "        df.index = X.index\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_stats_pipeline = Pipeline([\n",
    "    ('group_stats_filler', GroupStatsFiller()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filled = group_stats_pipeline.fit_transform(X_train_stats)\n",
    "X_test_filled = group_stats_pipeline.transform(X_test_stats)\n",
    "X_val_filled = group_stats_pipeline.transform(X_val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _make_column_names_unique(df, base_agg_dict):\n",
    "    \"\"\"Append the corresponding key from base_agg_dict to each column name.\"\"\"\n",
    "    keys_order = list(base_agg_dict.keys())  # Maintain the order of base_agg_dict\n",
    "    new_columns = [f\"{col}_{keys_order[i % len(keys_order)]}\" for i, col in enumerate(df.columns)]\n",
    "\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_group_stats(df):\n",
    "    def _update_agg_dict(base_dict, suffix):\n",
    "        return {key: value.replace(\"mean\", f\"mean{suffix}\").replace(\"sum\", f\"sum{suffix}\") for key, value in base_dict.items()}\n",
    "\n",
    "    base_agg_dict = {\n",
    "        'Age': 'mean',\n",
    "        'VIP': 'mean',\n",
    "        'CryoSleep': 'mean',\n",
    "        'RoomService': 'sum',\n",
    "        'VRDeck': 'sum',\n",
    "        'ShoppingMall': 'sum',\n",
    "        'FoodCourt': 'sum'\n",
    "    }\n",
    "\n",
    "    # Passenger Group\n",
    "    group_stats_pass = df.groupby('passenger_group').agg(base_agg_dict).reset_index()\n",
    "    renamed_agg_dict_pass = _update_agg_dict(base_agg_dict, \"_gm_pass_group\")\n",
    "    df_pass = df[['passenger_group']].merge(group_stats_pass, on='passenger_group', how='left')\n",
    "    df_pass = df_pass.rename(columns=renamed_agg_dict_pass)\n",
    "    new_pass_cols = df_pass.drop(columns='passenger_group')\n",
    "\n",
    "    # Cabin Deck\n",
    "    group_stats_deck = df.groupby('cabin_deck').agg(base_agg_dict).reset_index()\n",
    "    renamed_agg_dict_deck = _update_agg_dict(base_agg_dict, \"_gm_cab_deck\")\n",
    "    df_deck = df[['cabin_deck']].merge(group_stats_deck, on='cabin_deck', how='left')\n",
    "    df_deck = df_deck.rename(columns=renamed_agg_dict_deck)\n",
    "    new_deck_cols = df_deck.drop(columns='cabin_deck')\n",
    "\n",
    "    # Cabin Side\n",
    "    group_stats_side = df.groupby('cabin_side').agg(base_agg_dict).reset_index()\n",
    "    renamed_agg_dict_side = _update_agg_dict(base_agg_dict, \"_gm_cab_side\")\n",
    "    df_side = df[['cabin_side']].merge(group_stats_side, on='cabin_side', how='left')\n",
    "    df_side = df_side.rename(columns=renamed_agg_dict_side)\n",
    "    new_side_cols = df_side.drop(columns='cabin_side')\n",
    "\n",
    "    # Concatenate New Features\n",
    "    new_features = pd.concat([new_pass_cols, new_deck_cols, new_side_cols], axis=1)\n",
    "    new_features.index = df.index\n",
    "\n",
    "    new_features = _make_column_names_unique(new_features, base_agg_dict)\n",
    "\n",
    "    return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.transformation import PowerTransformer\n",
    "\n",
    "log_transformer = FunctionTransformer(lambda x: np.log(x + 1e-9))\n",
    "reciprocal_transformer = FunctionTransformer(lambda x: np.reciprocal(x + 1e-1))\n",
    "power_transformer = PowerTransformer(exp=0.23)\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('lt', log_transformer, make_column_selector(dtype_include='float64')),\n",
    "        ('rt', reciprocal_transformer, make_column_selector(dtype_include='float64')),\n",
    "        ('pt', power_transformer, make_column_selector(dtype_include='float64')),\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=True,\n",
    "    n_jobs=-1\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = transformer.fit_transform(X_train_filled)\n",
    "X_test_transformed = transformer.transform(X_test_filled)\n",
    "X_val_transformed = transformer.transform(X_val_filled)\n",
    "\n",
    "X_train_transformed.index = X_train.index\n",
    "\n",
    "X_train_concat = pd.concat([X_train.select_dtypes('float64'), X_train_transformed], axis=1)\n",
    "X_test_concat = pd.concat([X_test.select_dtypes('float64'), X_test_transformed], axis=1)\n",
    "X_val_concat = pd.concat([X_val.select_dtypes('float64'), X_val_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = Pipeline([\n",
    "    ('quantile', QuantileTransformer(output_distribution='normal'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_array = transformer_pipeline.fit_transform(X_train_concat)\n",
    "X_test_transformed_array = transformer_pipeline.transform(X_test_concat)\n",
    "X_val_transformed_array = transformer_pipeline.transform(X_val_concat)\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed_array, columns=X_train_concat.columns)\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed_array, columns=X_test_concat.columns)\n",
    "X_val_transformed_df = pd.DataFrame(X_val_transformed_array, columns=X_val_concat.columns)\n",
    "\n",
    "X_train_transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dropped = X_train.drop(['PassengerId', 'passenger_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train_dropped, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = X_train_dropped.index\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['cabin_deck'])['Transported'].mean().plot(ax=axes[0], title='cabin_deck vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['cabin_side'])['Transported'].mean().plot(ax=axes[1], title='cabin_side vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['Destination'])['Transported'].mean().plot(ax=axes[2], title='Destination vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['HomePlanet'])['Transported'].mean().plot(ax=axes[3], title='HomePlanet vs Transported')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['cabin_deck'])['Transported'].std().plot(ax=axes[0], title='cabin_deck vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['cabin_side'])['Transported'].std().plot(ax=axes[1], title='cabin_side vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['Destination'])['Transported'].std().plot(ax=axes[2], title='Destination vs Transported')\n",
    "pd.concat([X_train_dropped, y_train], axis=1).groupby(['HomePlanet'])['Transported'].std().plot(ax=axes[3], title='HomePlanet vs Transported')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "categorical_cols = X_train_dropped.select_dtypes('object').columns\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('te', TargetEncoder(cols=categorical_cols, smoothing=0.3, min_samples_leaf=10)),\n",
    "])\n",
    "\n",
    "X_train_encoded = pipeline.fit_transform(X_train_dropped, y_train)\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = X_train_encoded.index\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['cabin_deck'])['Transported'].mean().plot(ax=axes[0], title='cabin_deck vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['cabin_side'])['Transported'].mean().plot(ax=axes[1], title='cabin_side vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['Destination'])['Transported'].mean().plot(ax=axes[2], title='Destination vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['HomePlanet'])['Transported'].mean().plot(ax=axes[3], title='HomePlanet vs Transported')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "categorical_cols = X_train_dropped.select_dtypes('object').columns\n",
    "\n",
    "cat_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('oh', OneHotEncoder(), make_column_selector(dtype_include='object')),\n",
    "        ('oe', OrdinalEncoder(), make_column_selector(dtype_include='object')),\n",
    "        ('te', TargetEncoder(cols=categorical_cols, smoothing=0.3, min_samples_leaf=10), make_column_selector(dtype_include='object')),\n",
    "        ('ss', StandardScaler(), make_column_selector(dtype_include=['float64', 'int'])),\n",
    "        ('quantile', QuantileTransformer(output_distribution='normal'), make_column_selector(dtype_include=['float64', 'int']))\n",
    "    ],\n",
    "    remainder='drop'\n",
    ").set_output(transform='default')\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('cat_transformer', cat_transformer)\n",
    "])\n",
    "\n",
    "X_train_encoded_arr = cat_pipeline.fit_transform(X_train_dropped, y_train)\n",
    "X_test_encoded_arr = cat_pipeline.transform(X_test)\n",
    "X_val_encoded_arr = cat_pipeline.transform(X_val)\n",
    "\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded_arr, columns=cat_transformer.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded_arr, columns=cat_transformer.get_feature_names_out())\n",
    "X_val_encoded = pd.DataFrame(X_val_encoded_arr, columns=cat_transformer.get_feature_names_out())\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index = X_train_encoded.index\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 3))\n",
    "\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['oe__cabin_deck'])['Transported'].mean().plot(ax=axes[0], title='cabin_deck vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['oe__cabin_side'])['Transported'].mean().plot(ax=axes[1], title='cabin_side vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['oe__Destination'])['Transported'].mean().plot(ax=axes[2], title='Destination vs Transported')\n",
    "pd.concat([X_train_encoded, y_train], axis=1).groupby(['oe__HomePlanet'])['Transported'].mean().plot(ax=axes[3], title='HomePlanet vs Transported')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train_transformed_df,X_train_encoded], axis=1)\n",
    "X_test_final = pd.concat([X_test_transformed_df,X_test_encoded], axis=1)\n",
    "X_val_final = pd.concat([X_val_transformed_df,X_val_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.index = X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model = joblib.load('best_gbc_model.pkl')\n",
    "lr_model = joblib.load('best_lr_model.pkl')\n",
    "svc_model = joblib.load('best_svc_model.pkl')\n",
    "xgb_model = joblib.load('best_xgb_model.pkl')\n",
    "stacking_clf = joblib.load('stacking_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(model, X_val, y_val):\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    eval_accuracy = accuracy_score(y_val, y_pred, )\n",
    "\n",
    "    return eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(stacking_clf, X_val_final, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(gbc_model, X_val_final, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(xgb_model, X_val_final, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(lr_model, X_val_final, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_check(svc_model, X_val_final, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.to_parquet(path / 'X_train.pq')\n",
    "X_test_final.to_parquet(path / 'X_test.pq')\n",
    "X_val_final.to_parquet(path / 'X_val.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
