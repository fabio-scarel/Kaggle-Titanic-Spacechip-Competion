{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07caa507",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410211c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# import keras_tuner as kt\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.preprocessing as prepoc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import randint, uniform, reciprocal, expon\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for building and training neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from hyperopt import STATUS_OK, STATUS_FAIL, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbf659",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (Path('..') / 'Kaggle-Titanic-Spacechip-Competion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ccb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path / 'train.csv')\n",
    "test_data = pd.read_csv(path / 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4859fa",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20fb37",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c8ed5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 6))  # Create a figure and a 1x2 subplot\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0][0].scatter(treated_train_data['Transported'], treated_train_data['VRDeck'])\n",
    "axs[0][0].set_xlabel('Transported')\n",
    "axs[0][0].set_ylabel('VRDeck')\n",
    "axs[0][0].set_title('Scatter Plot: Transported vs VRDeck')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[0][1].scatter(treated_train_data['Transported'], treated_train_data['ShoppingMall'])\n",
    "axs[0][1].set_xlabel('Transported')\n",
    "axs[0][1].set_ylabel('ShoppingMall')\n",
    "axs[0][1].set_title('Scatter Plot: Transported vs ShoppingMall')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1][0].scatter(treated_train_data['Transported'], treated_train_data['Spa'])\n",
    "axs[1][0].set_xlabel('Transported')\n",
    "axs[1][0].set_ylabel('Spa')\n",
    "axs[1][0].set_title('Scatter Plot: Transported vs Spa')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1][1].scatter(treated_train_data['Transported'], treated_train_data['RoomService'])\n",
    "axs[1][1].set_xlabel('Transported')\n",
    "axs[1][1].set_ylabel('RoomService')\n",
    "axs[1][1].set_title('Scatter Plot: Transported vs RoomService')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[2][0].scatter(treated_train_data['Transported'], treated_train_data['total_spent'])\n",
    "axs[2][0].set_xlabel('Transported')\n",
    "axs[2][0].set_ylabel('total_spent')\n",
    "axs[2][0].set_title('Scatter Plot: Transported vs Total Spent')\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[2][1].scatter(treated_train_data['Transported'], treated_train_data['FoodCourt'])\n",
    "axs[2][1].set_xlabel('Transported')\n",
    "axs[2][1].set_ylabel('FoodCourt')\n",
    "axs[2][1].set_title('Scatter Plot: Transported vs FoodCourt')\n",
    "\n",
    "plt.tight_layout()  # Adjust the padding between and around the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92b30c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drop_ix_food = treated_train_data[(treated_train_data['FoodCourt']>20000)].index\n",
    "drop_ix_shopping = treated_train_data[(treated_train_data['ShoppingMall']>10000)].index\n",
    "\n",
    "treated_train_data = treated_train_data.drop(drop_ix_food)\n",
    "treated_train_data = treated_train_data.drop(drop_ix_shopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bb0ce",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checking relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ac0b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "figsize = (1.5, 1.3)\n",
    "g = sns.PairGrid(train_data, hue='Transported', vars=numerical_columns, height=figsize[1], aspect=figsize[0]/figsize[1])\n",
    "g.map_lower(sns.scatterplot)\n",
    "legend = g.add_legend(loc='upper right', bbox_to_anchor=(0.65, 0.7))\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 0)):\n",
    "    g.axes[i, j].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cdb76",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train = treated_train_data['Transported'].astype(int)\n",
    "treated_train_data.drop('Transported', axis=1, inplace=True)\n",
    "treated_train_data.drop('PassengerId', axis=1, inplace=True)\n",
    "treated_train_data.drop('passenger_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9ff89",
   "metadata": {},
   "source": [
    "### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24806df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot histograms for each variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_columns), figsize=(12, 4))\n",
    "\n",
    "for i, var in enumerate(numerical_columns):\n",
    "    axes[i].hist(treated_train_data[var], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9df56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "normalizer_pipeline = Pipeline([\n",
    "    ('feature_processing', ColumnTransformer([\n",
    "        ('normalizer', PowerTransformer(method='yeo-johnson'), make_column_selector(dtype_include=float)),\n",
    "        ('encoder', OneHotEncoder(), make_column_selector(dtype_exclude=np.number))\n",
    "    ], remainder='passthrough')),\n",
    "])\n",
    "\n",
    "treated_train_data = pd.DataFrame(normalizer_pipeline.fit_transform(treated_train_data), \n",
    "                                  columns= [col.replace('normalizer__', '').replace('remainder__', '').replace('encoder__', '') for col in normalizer_pipeline.named_steps['feature_processing'].get_feature_names_out()])\n",
    "\n",
    "treated_test_data = pd.DataFrame(normalizer_pipeline.transform(treated_test_data), \n",
    "                                 columns= [col.replace('normalizer__', '').replace('remainder__', '').replace('encoder__', '') for col in normalizer_pipeline.named_steps['feature_processing'].get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each variable\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_columns), figsize=(12, 4))\n",
    "\n",
    "for i, var in enumerate(numerical_columns):\n",
    "    axes[i].hist(treated_train_data[var], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65188cf2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beaab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet(path / 'X_train.pq')\n",
    "X_test = pd.read_parquet(path / 'X_test.pq')\n",
    "X_val = pd.read_parquet(path / 'X_val.pq')\n",
    "\n",
    "pca_train = pd.read_parquet(path / 'pca_train.pq')\n",
    "pca_test = pd.read_parquet(path / 'pca_test.pq')\n",
    "pca_val = pd.read_parquet(path / 'pca_val.pq')\n",
    "\n",
    "y_train = pd.read_parquet(path / 'y_train.pq').Transported\n",
    "y_test = pd.read_parquet(path / 'y_test.pq').Transported\n",
    "y_val = pd.read_parquet(path / 'y_val.pq').Transported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c288b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"passenger_group_count_frequency_yj\",\n",
    "    \"passenger_group_ordinal_enc_equal_freq\",\n",
    "    \"passenger_group_count_frequency_equal_freq_ef_ordinal_yj\"\n",
    "]\n",
    "\n",
    "X_train = X_train.drop(columns, axis=1)\n",
    "X_test = X_test.drop(columns, axis=1)\n",
    "X_val = X_val.drop(columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pca_train\n",
    "X_test = pca_test\n",
    "X_val = pca_val\n",
    "\n",
    "y_train = y_train.iloc[pca_train.index]\n",
    "y_test = y_test.iloc[pca_test.index]\n",
    "y_val = y_val.iloc[pca_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        epochs=3,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5dc462",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/mlops_gbc_4\")\n",
    "\n",
    "signature = infer_signature(X_train, y_train)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 680, 830, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.003), np.log(0.0075)),\n",
    "    'max_depth': 4,\n",
    "    'subsample': hp.uniform('subsample', 0.92, 1),\n",
    "    'min_samples_split': 3,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': hp.uniform('max_features', 0.31, 0.37)\n",
    "}\n",
    "\n",
    "def train_model(params, epochs, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_samples_split'] = int(params['min_samples_split'])\n",
    "    params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        preds = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        eval_rmse = np.sqrt(mse)\n",
    "        eval_accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "        # Log hyperparameters and the evaluation metric\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "        mlflow.log_metric(\"eval_accuracy\", eval_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trials=Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    best_run = sorted(trials.results, key=lambda x:x[\"loss\"])[0]\n",
    "\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run['loss'])\n",
    "    mlflow.log_metric(\"eval_accuracy\", best_run['eval_accuracy'])\n",
    "\n",
    "    mlflow.sklearn.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    print(f\"Best Parameters: {best}\")\n",
    "    print(f\"Best eval_accuracy: {best_run['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7bbbf",
   "metadata": {},
   "source": [
    "logged_model = 'runs:/f90df2cba435421598e4bd2872fb0bfd/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "y_pred = loaded_model.predict(X_val)\n",
    "\n",
    "eval_accuracy = accuracy_score(y_val, y_pred)\n",
    "eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f3ac2",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment(\"/mlops_xgb_2\")\n",
    "\n",
    "signature = infer_signature(X_train, y_train)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 200, 1000, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.015), np.log(0.032)),\n",
    "    'max_depth': 4,\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 3, 6, 1),\n",
    "}\n",
    "\n",
    "def train_model(params, epochs, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "\n",
    "    model = XGBClassifier(**params, enable_categorical=True)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        preds = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        eval_rmse = np.sqrt(mse)\n",
    "        eval_accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "        # Log hyperparameters and the evaluation metric\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "        mlflow.log_metric(\"eval_accuracy\", eval_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trials=Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run['loss'])\n",
    "\n",
    "    mlflow.sklearn.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    print(f\"Best Parameters: {best}\")\n",
    "    print(f\"Best eval_accuracy: {best_run['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137ed5d",
   "metadata": {},
   "source": [
    "logged_model = 'runs:/b65a47d54de047dc87554b67a89186a0/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "y_pred = loaded_model.predict(X_val)\n",
    "\n",
    "eval_accuracy = accuracy_score(y_val, y_pred)\n",
    "eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f43679",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment(\"/mlops_xgb_2\")\n",
    "\n",
    "signature = infer_signature(X_train, y_train)\n",
    "\n",
    "space = {\n",
    "    'C': hp.loguniform('C', np.log(1e-4), np.log(1e2)),\n",
    "    'solver': hp.choice('solver', ['liblinear', 'saga']),\n",
    "    'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True, False]),\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced'])\n",
    "}\n",
    "\n",
    "def train_model(params, epochs, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    if params['solver'] == 'liblinear' and params['penalty'] == 'l1':\n",
    "        params['dual'] = False\n",
    "    elif params['solver'] == 'saga' and params['penalty'] == 'l1':\n",
    "        params['dual'] = False\n",
    "    else:\n",
    "        return {'loss': np.inf, 'status': STATUS_FAIL}\n",
    "\n",
    "    model = LogisticRegression(**params, max_iter=1000)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        preds = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        eval_rmse = np.sqrt(mse)\n",
    "        eval_accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "        # Log hyperparameters and the evaluation metric\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "        mlflow.log_metric(\"eval_accuracy\", eval_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trials=Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    best_run = sorted(trials.results, key=lambda x:x[\"loss\"])[0]\n",
    "\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run['loss'])\n",
    "\n",
    "    mlflow.sklearn.log_model(best_run[\"model\"], \"model\", signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bcf8a",
   "metadata": {},
   "source": [
    "logged_model = 'runs:/f90df2cba435421598e4bd2872fb0bfd/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "y_pred = loaded_model.predict(X_val)\n",
    "\n",
    "eval_accuracy = accuracy_score(y_val, y_pred)\n",
    "eval_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47553b13",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment(\"/mlops_svc_1\")\n",
    "\n",
    "signature = infer_signature(X_train, y_train)\n",
    "\n",
    "space_svc = {\n",
    "    'C': hp.uniform('svc__C', 0.1, 10.0),\n",
    "    'kernel': hp.choice('svc__kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': hp.choice('svc__gamma', ['scale', 'auto']),\n",
    "    'degree': hp.randint('svc__degree', 2, 5),\n",
    "}\n",
    "\n",
    "def train_model(params, epochs, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    model = SVC(**params)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        preds = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "        eval_rmse = np.sqrt(mse)\n",
    "        eval_accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "        # Log hyperparameters and the evaluation metric\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "        mlflow.log_metric(\"eval_accuracy\", eval_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trials=Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space_svc,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run['loss'])\n",
    "\n",
    "    mlflow.sklearn.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    print(f\"Best Parameters: {best}\")\n",
    "    print(f\"Best eval_accuracy: {best_run['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce9529",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import joblib\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "models = {}\n",
    "registered_models = client.search_registered_models()\n",
    "\n",
    "for model in registered_models:\n",
    "\n",
    "    latest_version = max(int(v.version) for v in model.latest_versions)\n",
    "    model_uri = f\"models:/{model.name}/{latest_version}\"\n",
    "\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "    joblib.dump(loaded_model, f'{model.name}.pkl')\n",
    "\n",
    "    models[model.name] = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b39c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stacking_clf = StackingClassifier(\n",
    "\n",
    "    estimators=[\n",
    "        ('xgb', models['best_xgb_model']),\n",
    "        ('lr', models['best_lr_model']),\n",
    "        ('svc', models['best_svc_model']),\n",
    "        ('gbc', models['best_gbc_model']),\n",
    "    ],\n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4551e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stacking_accuracy = cross_val_score(stacking_clf, X_val, y_val,\n",
    "                              scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac688b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(stacking_accuracy).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ed1f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stacking_clf.predict_proba(treated_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d73561",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(stacking_clf.predict(treated_test_data).astype(bool),test_passenger_id, columns=['Transported']).to_csv(path / 'red_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a96ba0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516261b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_array = X_train.to_numpy()\n",
    "y_train_array = y_train.to_numpy()\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_array, y_train_array, epochs=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fb252",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa5dc0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define layers\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation='relu')\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation='relu')\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "# Define input layer\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# Connect layers\n",
    "hidden1 = hidden_layer1(input_)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([input_, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# Compile model with loss function\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_array, y_train_array, epochs=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018244bd",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aed64c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    n_hidden = hp.Int('n_hidden', min_value=6, max_value=11, default=9)\n",
    "    n_neurons = hp.Int('n_neurons', min_value=60, max_value=80)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=2e-3,\n",
    "                            sampling = 'log')\n",
    "    optimizer = hp.Choice('optimizer', values=['sgd', 'adam', 'adagrad', 'adadelta', 'rmsprop', 'adamax', 'nadam'])\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adamax':\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b27263",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective='val_accuracy', max_trials=40, overwrite=True,\n",
    "    directory='my_kaggle_comp', project_name='my_rnd_search')\n",
    "\n",
    "random_search_tuner.search(X_train, y_train, epochs=40,\n",
    "                          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c82f59",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top3_models = random_search_tuner.get_best_models(num_models=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96c70d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model=top3_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb525af2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
    "for param in top3_params:\n",
    "    print(param.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2543a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top3_trials = random_search_tuner.oracle.get_best_trials(num_trials=3)\n",
    "for summ in top3_trials:\n",
    "    print(summ.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa158a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model.predict(treated_test_data).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39c5ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = best_model.predict(treated_test_data)\n",
    "print(predictions[:10])  # Print the first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93fb94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "binary_predictions = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2118b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f403ac0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(binary_predictions.astype(bool),test_passenger_id, columns=['Transported']).to_csv(path / 'pred_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655506aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ce5ca",
   "metadata": {},
   "source": [
    "# Testing out some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_data.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b43846",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=60, random_state=42, n_init=10) \n",
    "X_train['Cluster'] = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['RoomService/total_spent'] = X_train['RoomService']/X_train['total_spent']\n",
    "X_train['FoodCourt/total_spent'] = X_train['FoodCourt']/X_train['total_spent']\n",
    "X_train['ShoppingMall/total_spent'] = X_train['ShoppingMall']/X_train['total_spent']\n",
    "X_train['Spa/total_spent'] = X_train['Spa']/X_train['total_spent']\n",
    "X_train['VRDeck/total_spent'] = X_train['VRDeck']/X_train['total_spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(rnd_search_xgb.best_params_)\n",
    "\n",
    "print(rnd_search_xgb.best_score_)\n",
    "\n",
    "final_xgb = rnd_search_xgb.best_estimator_\n",
    "\n",
    "feature_importances = final_xgb['xgb'].feature_importances_\n",
    "\n",
    "important_features = sorted(zip(feature_importances,\n",
    "         X_train.columns),\n",
    "         reverse=True)\n",
    "\n",
    "features = []\n",
    "\n",
    "for feature in important_features[0:20]:\n",
    "    features.append(feature[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3837082",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ea4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([X_train, y_train], axis=1)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de2134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['RoomService/total_spent','FoodCourt/total_spent','ShoppingMall/total_spent','Spa/total_spent','VRDeck/total_spent']\n",
    "\n",
    "figsize = (1.5, 1.3)\n",
    "g = sns.PairGrid(df_concat, hue='Transported', vars=numerical_columns, height=figsize[1], aspect=figsize[0]/figsize[1])\n",
    "g.map_lower(sns.scatterplot)\n",
    "legend = g.add_legend(loc='upper right', bbox_to_anchor=(0.65, 0.7))\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(g.axes, 0)):\n",
    "    g.axes[i, j].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbcff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feat = []\n",
    "new_feat = []\n",
    "original_feat = X_train.columns[:-10]\n",
    "original_feat = X_train[original_feat]\n",
    "new_feat = X_train.columns[-10:]\n",
    "new_feat = X_train[new_feat]\n",
    "\n",
    "new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7f97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c1b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
